import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional

import feedparser

logger = logging.getLogger(__name__)


class RSSParser:
    """–ü–∞—Ä—Å–µ—Ä RSS –ª–µ–Ω—Ç –¥–ª—è Archive of Our Own"""

    def __init__(self, feed_urls: list, state_file: str = "bot_state.json"):
        self.feed_urls = feed_urls if isinstance(feed_urls, list) else [feed_urls]
        self.state_file = state_file
        self.processed_entries = self._load_state()

    def _load_state(self) -> set:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    return set(data.get("processed_entries", []))
            except (json.JSONDecodeError, FileNotFoundError) as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {e}")
        return set()

    def _save_state(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        try:
            data = {
                "processed_entries": list(self.processed_entries),
                "last_check": datetime.now().isoformat(),
            }
            with open(self.state_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {e}")

    def fetch_feed(self, feed_url: str) -> Optional[feedparser.FeedParserDict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—É"""
        try:
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ RSS –ª–µ–Ω—Ç—ã: {feed_url}")
            feed = feedparser.parse(feed_url)

            if feed.bozo:
                logger.warning(f"RSS –ª–µ–Ω—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏: {feed.bozo_exception}")

            if not feed.entries:
                logger.warning(f"RSS –ª–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞: {feed_url}")
                return None

            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(feed.entries)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {feed_url}")
            return feed

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ RSS –ª–µ–Ω—Ç—ã {feed_url}: {e}")
            return None

    def get_new_entries(self) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –≤—Å–µ—Ö RSS –ª–µ–Ω—Ç"""
        all_new_entries = []

        for feed_url in self.feed_urls:
            feed_url = feed_url.strip()
            if not feed_url:
                continue

            logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–µ–Ω—Ç—ã: {feed_url}")
            feed = self.fetch_feed(feed_url)
            if not feed:
                continue

            new_entries = []

            for entry in feed.entries:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º link –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                entry_id = entry.get("link", "")
                if not entry_id:
                    continue

                if entry_id not in self.processed_entries:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
                    entry_data = {
                        "id": entry_id,
                        "title": entry.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                        "link": entry_id,
                        "description": entry.get("summary", ""),
                        "author": entry.get("author", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"),
                        "published": entry.get("published", ""),
                        "tags": self._extract_tags(entry),
                        "source_feed": feed_url,
                        "raw_entry": entry,
                    }

                    new_entries.append(entry_data)
                    self.processed_entries.add(entry_id)

            if new_entries:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(new_entries)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ {feed_url}")
                all_new_entries.extend(new_entries)

        if all_new_entries:
            logger.info(
                f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_new_entries)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π "
                f"–∏–∑ {len(self.feed_urls)} –ª–µ–Ω—Ç"
            )
            self._save_state()

        return all_new_entries

    def _extract_tags(self, entry) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏ –∏–∑ –∑–∞–ø–∏—Å–∏"""
        tags = []
        import html
        import re

        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–≥–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
        if hasattr(entry, "tags"):
            for tag in entry.tags:
                # –û—á–∏—â–∞–µ–º —Ç–µ–≥ –æ—Ç HTML
                clean_tag = html.unescape(tag.term)
                clean_tag = re.sub(r"<[^>]+>", "", clean_tag)
                clean_tag = clean_tag.strip()
                if clean_tag:
                    tags.append(clean_tag)

        # –ò—â–µ–º —Ç–µ–≥–∏ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        description = entry.get("summary", "")
        if "tags:" in description.lower():
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
            tag_matches = re.findall(r"tags?:?\s*([^\n]+)", description, re.IGNORECASE)
            for match in tag_matches:
                for tag in match.split(","):
                    clean_tag = html.unescape(tag.strip())
                    clean_tag = re.sub(r"<[^>]+>", "", clean_tag)
                    clean_tag = clean_tag.strip()
                    if clean_tag:
                        tags.append(clean_tag)

        return [
            tag for tag in tags if tag and len(tag) < 100
        ]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–≥–æ–≤

    def _extract_metadata(self, description: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""
        import html
        import re

        from bs4 import BeautifulSoup

        metadata = {}

        try:
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(description, "html.parser")

            # –ò—â–µ–º –≤—Å–µ li —ç–ª–µ–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            li_elements = soup.find_all("li")

            for li in li_elements:
                text = li.get_text().strip()

                # –§–∞–Ω–¥–æ–º
                if text.startswith("Fandoms:"):
                    links = li.find_all("a")
                    if links:
                        fandoms = [link.get_text().strip() for link in links]
                        metadata["fandom"] = ", ".join(fandoms)

                # –†–µ–π—Ç–∏–Ω–≥
                elif text.startswith("Rating:"):
                    links = li.find_all("a")
                    if links:
                        ratings = [link.get_text().strip() for link in links]
                        metadata["rating"] = ", ".join(ratings)

                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
                elif text.startswith("Categories:"):
                    links = li.find_all("a")
                    if links:
                        categories = [link.get_text().strip() for link in links]
                        metadata["category"] = ", ".join(categories)

                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                elif text.startswith("Warnings:"):
                    links = li.find_all("a")
                    if links:
                        warnings = [link.get_text().strip() for link in links]
                        metadata["warnings"] = ", ".join(warnings)

                # –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
                elif text.startswith("Characters:"):
                    links = li.find_all("a")
                    if links:
                        characters = [link.get_text().strip() for link in links]
                        metadata["characters"] = ", ".join(characters)

                # –ü–µ–π—Ä–∏–Ω–≥–∏
                elif text.startswith("Relationships:"):
                    links = li.find_all("a")
                    if links:
                        relationships = [link.get_text().strip() for link in links]
                        metadata["relationships"] = ", ".join(relationships)

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏
                elif text.startswith("Additional Tags:"):
                    links = li.find_all("a")
                    if links:
                        tags = [link.get_text().strip() for link in links]
                        metadata["additional_tags"] = ", ".join(tags)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            words_match = re.search(r"Words:\s*(\d+)", description)
            if words_match:
                metadata["words"] = words_match.group(1)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (—Å–∞–º–º–∞—Ä–∏) - —Ç–µ–∫—Å—Ç –º–µ–∂–¥—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏
            paragraphs = soup.find_all("p")
            summary_parts = []
            for p in paragraphs:
                text = p.get_text().strip()
                if text and not any(
                    keyword in text.lower()
                    for keyword in [
                        "words:",
                        "chapters:",
                        "language:",
                        "fandoms:",
                        "rating:",
                        "warnings:",
                        "categories:",
                        "characters:",
                        "relationships:",
                        "additional tags:",
                    ]
                ):
                    summary_parts.append(text)

            if summary_parts:
                summary_text = " ".join(summary_parts)
                # –£–±–∏—Ä–∞–µ–º "by <author_name>" –∏–∑ —Å–∞–º–º–∞—Ä–∏
                summary_text = re.sub(
                    r"^by\s+[^.]*\.?\s*", "", summary_text, flags=re.IGNORECASE
                ).strip()
                if summary_text.startswith("by "):
                    summary_text = summary_text[3:].strip()
                metadata["summary"] = summary_text

        except Exception as e:
            # –ï—Å–ª–∏ BeautifulSoup –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π regex
            import html

            clean_desc = html.unescape(description)

            # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            patterns = {
                "fandom": r"Fandoms:\s*<[^>]*>([^<]+)</a>",
                "rating": r"Rating:\s*<[^>]*>([^<]+)</a>",
                "category": r"Categories:\s*<[^>]*>([^<]+)</a>",
                "warnings": r"Warnings:\s*<[^>]*>([^<]+)</a>",
                "characters": r"Characters:\s*<[^>]*>([^<]+)</a>",
                "relationships": r"Relationships:\s*<[^>]*>([^<]+)</a>",
                "additional_tags": r"Additional Tags:\s*<[^>]*>([^<]+)</a>",
                "words": r"Words:\s*(\d+)",
            }

            for key, pattern in patterns.items():
                matches = re.findall(pattern, clean_desc, re.IGNORECASE)
                if matches:
                    if key == "words":
                        metadata[key] = matches[0]
                    else:
                        metadata[key] = ", ".join(matches)

        return metadata

    def format_entry_for_telegram(self, entry: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∑–∞–ø–∏—Å—å –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram"""
        import html
        import re

        # –û—á–∏—â–∞–µ–º –≤—Å–µ –ø–æ–ª—è –æ—Ç HTML
        title = html.unescape(entry["title"])
        title = re.sub(r"<[^>]+>", "", title).strip()

        # –ó–∞–º–µ–Ω—è–µ–º –¥–æ–º–µ–Ω .org –Ω–∞ .gay –≤ —Å—Å—ã–ª–∫–µ
        link = entry["link"].replace("archiveofourown.org", "archiveofourown.gay")

        author = html.unescape(entry["author"])
        author = re.sub(r"<[^>]+>", "", author).strip()

        description = entry["description"]
        tags = entry["tags"]
        source_feed = entry.get("source_feed", "")

        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        clean_description = html.unescape(description)
        clean_description = re.sub(r"<[^>]+>", "", clean_description)
        clean_description = re.sub(r"\s+", " ", clean_description).strip()

        # –£–±–∏—Ä–∞–µ–º "by <author_name>" –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        clean_description = re.sub(
            r"^by\s+[^.]*\.?\s*", "", clean_description, flags=re.IGNORECASE
        ).strip()
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–±–∏—Ä–∞–µ–º "by" –µ—Å–ª–∏ –æ–Ω –æ—Å—Ç–∞–ª—Å—è
        if clean_description.startswith("by "):
            clean_description = clean_description[3:].strip()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        metadata = self._extract_metadata(description)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        message = f"<a href='{link}'><b>{title}</b></a>\n"
        message += f"üë§ <b>–ê–≤—Ç–æ—Ä:</b> {author}\n"

        # –§–∞–Ω–¥–æ–º
        if metadata.get("fandom"):
            message += f"üåç <b>–§–∞–Ω–¥–æ–º:</b> {metadata['fandom']}\n"

        # –†–µ–π—Ç–∏–Ω–≥
        if metadata.get("rating"):
            message += f"‚≠ê <b>–†–µ–π—Ç–∏–Ω–≥:</b> {metadata['rating']}\n"

        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
        if metadata.get("category"):
            message += f"üìÇ <b>–ö–∞—Ç–µ–≥–æ—Ä–∏—è:</b> {metadata['category']}\n"

        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
        if (
            metadata.get("warnings")
            and metadata["warnings"] != "No Archive Warnings Apply"
        ):
            message += f"‚ö†Ô∏è <b>–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:</b> {metadata['warnings']}\n"

        # –ü–µ–π—Ä–∏–Ω–≥–∏ –∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏
        relationships = metadata.get("relationships", "")
        characters = metadata.get("characters", "")
        if relationships or characters:
            # –í—ã–¥–µ–ª—è–µ–º –ø–µ–π—Ä–∏–Ω–≥–∏ –∂–∏—Ä–Ω—ã–º
            if relationships:
                relationships = f"<b>{relationships}</b>"
            message += f"üíï <b>–ü–µ–π—Ä–∏–Ω–≥ –∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏:</b> {relationships}, {characters}\n"

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        if metadata.get("words"):
            message += f"üìù <b>–ö–æ–ª-–≤–æ —Å–ª–æ–≤:</b> {metadata['words']}\n"

        # –¢–µ–≥–∏
        if metadata.get("additional_tags"):
            message += f"üè∑Ô∏è <b>–¢—ç–≥–∏:</b> {metadata['additional_tags']}\n"

        # –û–ø–∏—Å–∞–Ω–∏–µ
        if metadata.get("summary"):
            message += f"üìñ <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {metadata['summary']}"
        elif clean_description:
            message += f"üìñ <b>–û–ø–∏—Å–∞–Ω–∏–µ:</b> {clean_description}"

        return message

    def _extract_feed_name(self, feed_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–Ω—è—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–µ–Ω—Ç—ã –∏–∑ URL"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            from config import Config

            description = Config.get_feed_description(feed_url)
            if description != "Unknown Feed":
                return description

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ URL
            if "tags/" in feed_url:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Ç–µ–≥–∞
                parts = feed_url.split("tags/")[1].split("/")[0]
                return f"Tag ID: {parts}"
            elif "users/" in feed_url:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                parts = feed_url.split("users/")[1].split("/")[0]
                return f"–ê–≤—Ç–æ—Ä: {parts}"
            else:
                return "AO3"
        except Exception:
            return "AO3"
